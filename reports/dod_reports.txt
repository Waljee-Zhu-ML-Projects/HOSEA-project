2022/01/01 - 2022/03/30
-----------------------

We performed a collection of analyses to study the sensitivity and performance of our proposed method. First, we compared the risk prediction from our model to other known methods in the litterature (HUNT study, Kunzmann et al., various screening guidelines) and found that significantly outperform all of these methods. Second, we studied how the data collection window influences the performance: the original training is done with records starting one year prior to the prediction date up to five years before that data, for a total of four years of data. We checked whether not going that far in time impacts the results and found that we retain most of the predictive power as long as we go three years back (for a total of two years of data). We also checked whether predicting farther in the future yields worse predictive performance and found a drop in performance at the third year. Third, and in a similar vein, we studied our model's performance when stratifying predicted risk by the cancer stage: we found that all stages share a similar risk distribution, which indicates we are equally able to predict cancer at earlier stages. Fourth, we investigated the predictive performance when stratifying the test set by sex and race: while this comparison is hard due to some groups being much less represented than others, we found no clear loss of performance across identity groups. Fifth, we studied our model's performance when stratifying by cancer type, i.e., EAC or EGJAC. We found significantly better performance for EGJAC than for EAC and we are currently trying to understand the reasons for this difference. Sixth, we started investigating dropping variables from the model to obtain a more interpretable model which retains the same predictive power. In particular, we identified some highly-correlated variables, both statistically and clinically-informed, and checked whether we kept the same performance while dropping some of them.  Seventh, we started investigating whether the change from ICD9 to ICD10 affects our performance. In particular, our method will be applied to ICD10-only patients while it was trained on both (some patients ICD9-only, some have both.) Our initial data does not provide much data in the ICD10 period, so we will be going back at that analysis if and when our admendment is approved. Eight, we started investigating the impact of missing values on predicted risk: in particular, which features will change the prediction the most under repeated imputation and whether multiple imputation can mitigate this effect. 

We studied the impact of each features to the predictive model using variable importance, partial dependence plots and SHAP values. We found that known risk factors affect the predictions as expected: risk increases with age up until 60-65 yo; higher BMI and weight is associated with higher risk; White males have higher risk than other groups; GERD and smoking increase risk. The interpretation of the impact of medication and lab results on predicted risk is not as straightforward however, and this is a ongoing task.


2022/04/01 - 2022/06/30
-----------------------

We performed additional analyses to study the performance, robustness and sensitivity of our model, as well as to further understand which variable affect the predicted risk and how. 

First, we performed an additional comparison to known methods (HUNT study, Kunzmann et al., various screening guidelines) where we now consider a testing set closer to what we would expect un the general US population. In particular, we constructed testing sets with appropriate proportion of women (~50%) and with appropriate incidence per sex (~8:1 ratio mean to women). We found that our method still outperformed the other approaches by a large margin in terms of discrimination. We also studied the calibration of our predicted risk by age and birth year and did not found clear marks of miscalibration.

Second, while inspecting the effect of age on the predicted risk, we found some unexpected behaviour: while we the predicted risk generally increases up to 60-65 yo, we notice a decrease beyond that age. From prior knowledge and various analyses, we determined that a likely explanation was the "birth-cohort effect" where earlier cohort exhibit lower incidence rates. In particular, older patients in our study come from ealier cohort so we expect that effect to take place. In order to test this hypothesis, we included birth year as a feature of the model and found that the decrease in predicted risk for older patients was no longer present. Since our predictive tool will be used with combination of age and birth year that the model will not have seen, we decided not to inlcude birth year in the final model; testing performance did not significantly increase anyway. 

Third, upon studying the effect of GERD on the predicted risk, we noticed that our calculation of GERD was not symmetrical between ICD9 and ICD 10 patients. Indeed, ICD 9 patients were coded as having had GERD in the past whenever there was an ICD 9 code for heartburn of reflux without esophagitis, while ICD 10 patients only used codes for heartburn. This was originally done as there was some worry that the ICD 10 code for reflux without esophagitis required an EGD, which would defeat the purpose of the prediction tool. We inspected incidence of ICD codes for reflux and found matching rates so we decided to change the GERD definition for ICD 10 to now include reflux without esophagitis. GERD is ow one of the most important  to ble in the model, as was clinically expected, and the model performance improved significantly from an AUC of 0.85 to 0.90. 

We implemented our prediction tool in a private R package to be used in the pilot study. The package contains a few functions performing data processing, data validation and risk prediction. 

Among the tasks currently under progress, we note: studying the difference between EAC and EGJAC predicted risk in the separate models, preparing the ICD9/10 comparison when new data will be available, discussing threshold selection and risk reporting, interpreting variables better, especially lab results and medication. 



2022/07/01 - 2022/09/30
-----------------------

While inspecting the difference in performance between our EAC and EGJAC models, we uncovered issues with our data. Somehow, lab results for EGJAC patients were incomplete which induced a much higher proportion of missing values, especially for CBC results. Then, it appears our fitted EGJAC model was able to reverse engineer the imputation: indeed, performance was inflated and the importance of these lab results was also over-emphasized. After reprocessing the cases and refitting the model, we now have three models of similar performances. 

This investigative process also uncovered an important bug that inadvertantly crept in the code. In particular, when preparing the R package for eventual use, the default window for processing data was set to 4 years before index time to index time, as would be required for practical use. However, for building the model and futher analyses we needed to set it to 5 years prior to 1 year prior: that change was not made for fitting. That mistake yielded inflated performance as it is harder to predict case/control status a year prior to diagnosis that at the diagnosis it self. In terms of AUC, we originally had ANY 0.90, EAC 0.86 and EGJAC 0.95 which reduced to ANY 0.82, EAC 0.80 and EGJAC 0.94 after rolling back to 5 years to 1 year. Then, after the EGJAC lab results update, we obtained AUCs of ANY 0.77, EAC 0.77 and EGJAC 0.74. In comparison, the Kunzmann method applied to our dataset has an AUC of around 0.65 and the HUNT study method has an AUC of around 0.60. 

We repeated all the previous sensitivity analyses and found similar conclusions as before. We find that the all predictors known to be risk factors, i.e., age, sex, race, GERD, bmi/weight and smoking status, all are amoung the top 10 most important features of the models. Visual inspection of partial dependence agrees with the known associations.

Early on, we discarded the use of regression imputation as it produced inflated preformance on patiets with missing values, suggesting that the model is learning the imputation. We found that simple random sampling from the training set protects best against that phenomenon. The issue with EGJAC discussed above led us to reconsider these conclusions. We are currently working on using MICE imputation instead: early results seem to indicate that it can improve the performance of our models while guarding against over-fitting to missing values.


